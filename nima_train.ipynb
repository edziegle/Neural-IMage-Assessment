{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file - model.py\n",
    "Implements the aesthemic model and emd loss used in paper.\n",
    "Copyright (C) Yunxiao Shi 2017 - 2021\n",
    "NIMA is released under the MIT license. See LICENSE for the fill license text.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NIMA(nn.Module):\n",
    "\n",
    "    \"\"\"Neural IMage Assessment model by Google\"\"\"\n",
    "    def __init__(self, base_model, num_classes=10):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.features = base_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(in_features=25088, out_features=num_classes),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def single_emd_loss(p, q, r=2):\n",
    "    \"\"\"\n",
    "    Earth Mover's Distance of one sample\n",
    "    Args:\n",
    "        p: true distribution of shape num_classes × 1\n",
    "        q: estimated distribution of shape num_classes × 1\n",
    "        r: norm parameter\n",
    "    \"\"\"\n",
    "    assert p.shape == q.shape, \"Length of the two distribution must be the same\"\n",
    "    length = p.shape[0]\n",
    "    emd_loss = 0.0\n",
    "    for i in range(1, length + 1):\n",
    "        emd_loss += torch.abs(sum(p[:i] - q[:i])) ** r\n",
    "    return (emd_loss / length) ** (1. / r)\n",
    "\n",
    "\n",
    "def emd_loss(p, q, r=2):\n",
    "    \"\"\"\n",
    "    Earth Mover's Distance on a batch\n",
    "    Args:\n",
    "        p: true distribution of shape mini_batch_size × num_classes × 1\n",
    "        q: estimated distribution of shape mini_batch_size × num_classes × 1\n",
    "        r: norm parameters\n",
    "    \"\"\"\n",
    "    assert p.shape == q.shape, \"Shape of the two distribution batches must be the same.\"\n",
    "    mini_batch_size = p.shape[0]\n",
    "    loss_vector = []\n",
    "    for i in range(mini_batch_size):\n",
    "        loss_vector.append(single_emd_loss(p[i], q[i], r=r))\n",
    "    return sum(loss_vector) / mini_batch_size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file - dataset.py\n",
    "Customized dataset class to loop through the AVA dataset and apply needed image augmentations for training.\n",
    "Copyright (C) Yunxiao Shi 2017 - 2021\n",
    "NIMA is released under the MIT license. See LICENSE for the fill license text.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class AVADataset(data.Dataset):\n",
    "    \"\"\"AVA dataset\n",
    "    Args:\n",
    "        csv_file: a 11-column csv_file, column one contains the names of image files, column 2-11 contains the empiricial distributions of ratings\n",
    "        root_dir: directory to the images\n",
    "        transform: preprocessing and augmentation of the training images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.annotations.iloc[idx, 0]) + '.jpg')\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        bucket = \"arc-photo-video-mlops\"\n",
    "        file_name = str(self.annotations.iloc[idx, 0]) + '.jpg'\n",
    "\n",
    "        data_key = f\"AVA Dataset/images/images/{file_name}\"\n",
    "\n",
    "        image = Image.open(BytesIO(\n",
    "            s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n",
    "        )).convert('RGB')\n",
    "        # image = Image.open(img_name).convert('RGB')\n",
    "        annotations = self.annotations.iloc[idx, 1:].to_numpy()\n",
    "        annotations = annotations.astype('float').reshape(-1, 1)\n",
    "        sample = {'img_id': img_name, 'image': image, 'annotations': annotations}\n",
    "        print(f\"Loading {sample}\")\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def nima_go(config):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    base_model = models.vgg16(pretrained=True)\n",
    "    model = NIMA(base_model)\n",
    "\n",
    "    if config.warm_start:\n",
    "        model.load_state_dict(torch.load(os.path.join(config.ckpt_path, 'epoch-%d.pth' % config.warm_start_epoch)))\n",
    "        print('Successfully loaded model epoch-%d.pth' % config.warm_start_epoch)\n",
    "\n",
    "    if config.multi_gpu:\n",
    "        model.features = torch.nn.DataParallel(model.features, device_ids=config.gpu_ids)\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    conv_base_lr = config.conv_base_lr\n",
    "    dense_lr = config.dense_lr\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': model.features.parameters(), 'lr': conv_base_lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': dense_lr}],\n",
    "        momentum=0.9\n",
    "        )\n",
    "\n",
    "    param_num = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_num += param.numel()\n",
    "    print('Trainable params: %.2f million' % (param_num / 1e6))\n",
    "\n",
    "    if config.train:\n",
    "        trainset = AVADataset(csv_file=config.train_csv_file, root_dir=config.img_path, transform=train_transform)\n",
    "        valset = AVADataset(csv_file=config.val_csv_file, root_dir=config.img_path, transform=val_transform)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=config.train_batch_size,\n",
    "            shuffle=True, num_workers=config.num_workers)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=config.val_batch_size,\n",
    "            shuffle=False, num_workers=config.num_workers)\n",
    "        # for early stopping\n",
    "        count = 0\n",
    "        init_val_loss = float('inf')\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(config.warm_start_epoch, config.epochs):\n",
    "            batch_losses = []\n",
    "            for i, data in enumerate(train_loader):\n",
    "                images = data['image'].to(device)\n",
    "                labels = data['annotations'].to(device).float()\n",
    "                outputs = model(images)\n",
    "                outputs = outputs.view(-1, 10, 1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = emd_loss(labels, outputs)\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                print('Epoch: %d/%d | Step: %d/%d | Training EMD loss: %.4f' % (epoch + 1, config.epochs, i + 1, len(trainset) // config.train_batch_size + 1, loss.data[0]))\n",
    "                writer.add_scalar('batch train loss', loss.data[0], i + epoch * (len(trainset) // config.train_batch_size + 1))\n",
    "\n",
    "            avg_loss = sum(batch_losses) / (len(trainset) // config.train_batch_size + 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            print('Epoch %d mean training EMD loss: %.4f' % (epoch + 1, avg_loss))\n",
    "\n",
    "            # exponetial learning rate decay\n",
    "            if config.decay:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    conv_base_lr = conv_base_lr * config.lr_decay_rate ** ((epoch + 1) / config.lr_decay_freq)\n",
    "                    dense_lr = dense_lr * config.lr_decay_rate ** ((epoch + 1) / config.lr_decay_freq)\n",
    "                    optimizer = optim.SGD([\n",
    "                        {'params': model.features.parameters(), 'lr': conv_base_lr},\n",
    "                        {'params': model.classifier.parameters(), 'lr': dense_lr}],\n",
    "                        momentum=0.9\n",
    "                    )\n",
    "\n",
    "            # do validation after each epoch\n",
    "            batch_val_losses = []\n",
    "            for data in val_loader:\n",
    "                images = data['image'].to(device)\n",
    "                labels = data['annotations'].to(device).float()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(images)\n",
    "                outputs = outputs.view(-1, 10, 1)\n",
    "                val_loss = emd_loss(labels, outputs)\n",
    "                batch_val_losses.append(val_loss.item())\n",
    "            avg_val_loss = sum(batch_val_losses) / (len(valset) // config.val_batch_size + 1)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print('Epoch %d completed. Mean EMD loss on val set: %.4f.' % (epoch + 1, avg_val_loss))\n",
    "            writer.add_scalars('epoch losses', {'epoch train loss': avg_loss, 'epoch val loss': avg_val_loss}, epoch + 1)\n",
    "\n",
    "            # Use early stopping to monitor training\n",
    "            if avg_val_loss < init_val_loss:\n",
    "                init_val_loss = avg_val_loss\n",
    "                # save model weights if val loss decreases\n",
    "                print('Saving model...')\n",
    "                if not os.path.exists(config.ckpt_path):\n",
    "                    os.makedirs(config.ckpt_path)\n",
    "                torch.save(model.state_dict(), os.path.join(config.ckpt_path, 'epoch-%d.pth' % (epoch + 1)))\n",
    "                print('Done.\\n')\n",
    "                # reset count\n",
    "                count = 0\n",
    "            elif avg_val_loss >= init_val_loss:\n",
    "                count += 1\n",
    "                if count == config.early_stopping_patience:\n",
    "                    print('Val EMD loss has not decreased in %d epochs. Training terminated.' % config.early_stopping_patience)\n",
    "                    break\n",
    "\n",
    "        print('Training completed.')\n",
    "\n",
    "\n",
    "    if config.test:\n",
    "        model.eval()\n",
    "        # compute mean score\n",
    "        test_transform = val_transform\n",
    "        testset = AVADataset(csv_file=config.test_csv_file, root_dir=config.img_path, transform=val_transform)\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=config.test_batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "        mean_preds = []\n",
    "        std_preds = []\n",
    "        for data in test_loader:\n",
    "            image = data['image'].to(device) # fixme\n",
    "            output = model(image)\n",
    "            output = output.view(10, 1)\n",
    "            predicted_mean, predicted_std = 0.0, 0.0\n",
    "            for i, elem in enumerate(output, 1):\n",
    "                predicted_mean += i * elem\n",
    "            for j, elem in enumerate(output, 1):\n",
    "                predicted_std += elem * (j - predicted_mean) ** 2\n",
    "            predicted_std = predicted_std ** 0.5\n",
    "            mean_preds.append(predicted_mean)\n",
    "            std_preds.append(predicted_std)\n",
    "        # Do what you want with predicted and std..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# The original NIMa codebase uses argparse, we simulate an argparse object here.\n",
    "config = type('test', (object,), {})()\n",
    "config.train = True\n",
    "config.train_csv_file = \"./train_labels.csv\"\n",
    "config.val_csv_file = \"./val_labels.csv\"\n",
    "config.conv_base_lr = 5e-4\n",
    "config.dense_lr = 5e-3\n",
    "config.lr_decay_rate = 0.95\n",
    "config.lr_decay_freq = 10\n",
    "config.train_batch_size = 64\n",
    "config.val_batch_size = 64\n",
    "config.decay = True\n",
    "config.ckpt_path = \"./ckpts\"\n",
    "config.epochs = 100\n",
    "config.early_stopping_patience = 10\n",
    "config.num_workers = 2\n",
    "config.warm_start = False # Use this option to resume from saved checkpoints\n",
    "config.warm_start_epoch = 0\n",
    "config.multi_gpu = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "os.environ[\"AWS_PROFILE\"]=\"video-refresh\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zieglere/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/zieglere/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 14.97 million\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/zieglere/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/zieglere/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'AVADataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m config\u001B[38;5;241m.\u001B[39mimg_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3://sagemaker-studio-754610378112-lonxocmwmve/AVA Dataset/images/images\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mnima_go\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mnima_go\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mwarm_start_epoch, config\u001B[38;5;241m.\u001B[39mepochs):\n\u001B[1;32m     77\u001B[0m     batch_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     79\u001B[0m         images \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     80\u001B[0m         labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mannotations\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:444\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:390\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1077\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1070\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1074\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1077\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1079\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/context.py:284\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_launch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Neural-IMage-Assessment/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetbuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "config.img_path = \"s3://sagemaker-studio-754610378112-lonxocmwmve/AVA Dataset/images/images\"\n",
    "nima_go(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
