{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file - model.py\n",
    "Implements the aesthemic model and emd loss used in paper.\n",
    "Copyright (C) Yunxiao Shi 2017 - 2021\n",
    "NIMA is released under the MIT license. See LICENSE for the fill license text.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NIMA(nn.Module):\n",
    "\n",
    "    \"\"\"Neural IMage Assessment model by Google\"\"\"\n",
    "    def __init__(self, base_model, num_classes=10):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.features = base_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(in_features=25088, out_features=num_classes),\n",
    "            nn.Softmax())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def single_emd_loss(p, q, r=2):\n",
    "    \"\"\"\n",
    "    Earth Mover's Distance of one sample\n",
    "    Args:\n",
    "        p: true distribution of shape num_classes × 1\n",
    "        q: estimated distribution of shape num_classes × 1\n",
    "        r: norm parameter\n",
    "    \"\"\"\n",
    "    assert p.shape == q.shape, \"Length of the two distribution must be the same\"\n",
    "    length = p.shape[0]\n",
    "    emd_loss = 0.0\n",
    "    for i in range(1, length + 1):\n",
    "        emd_loss += torch.abs(sum(p[:i] - q[:i])) ** r\n",
    "    return (emd_loss / length) ** (1. / r)\n",
    "\n",
    "\n",
    "def emd_loss(p, q, r=2):\n",
    "    \"\"\"\n",
    "    Earth Mover's Distance on a batch\n",
    "    Args:\n",
    "        p: true distribution of shape mini_batch_size × num_classes × 1\n",
    "        q: estimated distribution of shape mini_batch_size × num_classes × 1\n",
    "        r: norm parameters\n",
    "    \"\"\"\n",
    "    assert p.shape == q.shape, \"Shape of the two distribution batches must be the same.\"\n",
    "    mini_batch_size = p.shape[0]\n",
    "    loss_vector = []\n",
    "    for i in range(mini_batch_size):\n",
    "        loss_vector.append(single_emd_loss(p[i], q[i], r=r))\n",
    "    return sum(loss_vector) / mini_batch_size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file - dataset.py\n",
    "Customized dataset class to loop through the AVA dataset and apply needed image augmentations for training.\n",
    "Copyright (C) Yunxiao Shi 2017 - 2021\n",
    "NIMA is released under the MIT license. See LICENSE for the fill license text.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class AVADataset(data.Dataset):\n",
    "    \"\"\"AVA dataset\n",
    "    Args:\n",
    "        csv_file: a 11-column csv_file, column one contains the names of image files, column 2-11 contains the empiricial distributions of ratings\n",
    "        root_dir: directory to the images\n",
    "        transform: preprocessing and augmentation of the training images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.annotations.iloc[idx, 0]) + '.jpg')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        annotations = self.annotations.iloc[idx, 1:].to_numpy()\n",
    "        annotations = annotations.astype('float').reshape(-1, 1)\n",
    "        sample = {'img_id': img_name, 'image': image, 'annotations': annotations}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def nima_go(config):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    base_model = models.vgg16(pretrained=True)\n",
    "    model = NIMA(base_model)\n",
    "\n",
    "    if config.warm_start:\n",
    "        model.load_state_dict(torch.load(os.path.join(config.ckpt_path, 'epoch-%d.pth' % config.warm_start_epoch)))\n",
    "        print('Successfully loaded model epoch-%d.pth' % config.warm_start_epoch)\n",
    "\n",
    "    if config.multi_gpu:\n",
    "        model.features = torch.nn.DataParallel(model.features, device_ids=config.gpu_ids)\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    conv_base_lr = config.conv_base_lr\n",
    "    dense_lr = config.dense_lr\n",
    "    optimizer = optim.SGD([\n",
    "        {'params': model.features.parameters(), 'lr': conv_base_lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': dense_lr}],\n",
    "        momentum=0.9\n",
    "        )\n",
    "\n",
    "    param_num = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_num += param.numel()\n",
    "    print('Trainable params: %.2f million' % (param_num / 1e6))\n",
    "\n",
    "    if config.train:\n",
    "        trainset = AVADataset(csv_file=config.train_csv_file, root_dir=config.img_path, transform=train_transform)\n",
    "        valset = AVADataset(csv_file=config.val_csv_file, root_dir=config.img_path, transform=val_transform)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=config.train_batch_size,\n",
    "            shuffle=True, num_workers=config.num_workers)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=config.val_batch_size,\n",
    "            shuffle=False, num_workers=config.num_workers)\n",
    "        # for early stopping\n",
    "        count = 0\n",
    "        init_val_loss = float('inf')\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(config.warm_start_epoch, config.epochs):\n",
    "            batch_losses = []\n",
    "            for i, data in enumerate(train_loader):\n",
    "                images = data['image'].to(device)\n",
    "                labels = data['annotations'].to(device).float()\n",
    "                outputs = model(images)\n",
    "                outputs = outputs.view(-1, 10, 1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = emd_loss(labels, outputs)\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                print('Epoch: %d/%d | Step: %d/%d | Training EMD loss: %.4f' % (epoch + 1, config.epochs, i + 1, len(trainset) // config.train_batch_size + 1, loss.data[0]))\n",
    "                writer.add_scalar('batch train loss', loss.data[0], i + epoch * (len(trainset) // config.train_batch_size + 1))\n",
    "\n",
    "            avg_loss = sum(batch_losses) / (len(trainset) // config.train_batch_size + 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            print('Epoch %d mean training EMD loss: %.4f' % (epoch + 1, avg_loss))\n",
    "\n",
    "            # exponetial learning rate decay\n",
    "            if config.decay:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    conv_base_lr = conv_base_lr * config.lr_decay_rate ** ((epoch + 1) / config.lr_decay_freq)\n",
    "                    dense_lr = dense_lr * config.lr_decay_rate ** ((epoch + 1) / config.lr_decay_freq)\n",
    "                    optimizer = optim.SGD([\n",
    "                        {'params': model.features.parameters(), 'lr': conv_base_lr},\n",
    "                        {'params': model.classifier.parameters(), 'lr': dense_lr}],\n",
    "                        momentum=0.9\n",
    "                    )\n",
    "\n",
    "            # do validation after each epoch\n",
    "            batch_val_losses = []\n",
    "            for data in val_loader:\n",
    "                images = data['image'].to(device)\n",
    "                labels = data['annotations'].to(device).float()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(images)\n",
    "                outputs = outputs.view(-1, 10, 1)\n",
    "                val_loss = emd_loss(labels, outputs)\n",
    "                batch_val_losses.append(val_loss.item())\n",
    "            avg_val_loss = sum(batch_val_losses) / (len(valset) // config.val_batch_size + 1)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print('Epoch %d completed. Mean EMD loss on val set: %.4f.' % (epoch + 1, avg_val_loss))\n",
    "            writer.add_scalars('epoch losses', {'epoch train loss': avg_loss, 'epoch val loss': avg_val_loss}, epoch + 1)\n",
    "\n",
    "            # Use early stopping to monitor training\n",
    "            if avg_val_loss < init_val_loss:\n",
    "                init_val_loss = avg_val_loss\n",
    "                # save model weights if val loss decreases\n",
    "                print('Saving model...')\n",
    "                if not os.path.exists(config.ckpt_path):\n",
    "                    os.makedirs(config.ckpt_path)\n",
    "                torch.save(model.state_dict(), os.path.join(config.ckpt_path, 'epoch-%d.pth' % (epoch + 1)))\n",
    "                print('Done.\\n')\n",
    "                # reset count\n",
    "                count = 0\n",
    "            elif avg_val_loss >= init_val_loss:\n",
    "                count += 1\n",
    "                if count == config.early_stopping_patience:\n",
    "                    print('Val EMD loss has not decreased in %d epochs. Training terminated.' % config.early_stopping_patience)\n",
    "                    break\n",
    "\n",
    "        print('Training completed.')\n",
    "\n",
    "\n",
    "    if config.test:\n",
    "        model.eval()\n",
    "        # compute mean score\n",
    "        test_transform = val_transform\n",
    "        testset = AVADataset(csv_file=config.test_csv_file, root_dir=config.img_path, transform=val_transform)\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=config.test_batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "        mean_preds = []\n",
    "        std_preds = []\n",
    "        for data in test_loader:\n",
    "            image = data['image'].to(device) # fixme\n",
    "            output = model(image)\n",
    "            output = output.view(10, 1)\n",
    "            predicted_mean, predicted_std = 0.0, 0.0\n",
    "            for i, elem in enumerate(output, 1):\n",
    "                predicted_mean += i * elem\n",
    "            for j, elem in enumerate(output, 1):\n",
    "                predicted_std += elem * (j - predicted_mean) ** 2\n",
    "            predicted_std = predicted_std ** 0.5\n",
    "            mean_preds.append(predicted_mean)\n",
    "            std_preds.append(predicted_std)\n",
    "        # Do what you want with predicted and std..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The original NIMa codebase uses argparse, we simulate an argparse object here.\n",
    "config = type('test', (object,), {})()\n",
    "# config.img_path = base_dir FIXME\n",
    "config.train = True\n",
    "config.train_csv_file = \"/content/gdrive/MyDrive/train_labels.csv\"  # FIXME\n",
    "config.val_csv_file = \"/content/gdrive/MyDrive/val_labels.csv\"  # FIXME\n",
    "config.conv_base_lr = 5e-4\n",
    "config.dense_lr = 5e-3\n",
    "config.lr_decay_rate = 0.95\n",
    "config.lr_decay_freq = 10\n",
    "config.train_batch_size = 128\n",
    "config.val_batch_size = 128\n",
    "config.decay = True\n",
    "config.ckpt_path = \"./ckpts\" # TODO: GDrive location\n",
    "config.epochs = 100\n",
    "config.early_stopping_patience = 10\n",
    "config.num_workers = 2\n",
    "config.warm_start = False # Use this option to resume from saved checkpoints\n",
    "config.warm_start_epoch = 0\n",
    "config.multi_gpu = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nima_go(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
